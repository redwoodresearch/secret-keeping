{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Petri Eval Results\n",
    "\n",
    "Load transcripts, classify whether the auditor detected the hidden behavior, and plot detection rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.utils import load_prompt_file\n",
    "from src import api, Prompt, ChatMessage, MessageRole\n",
    "from src.viz import plot_hierarchical_bars\n",
    "\n",
    "# ---------- Load transcripts from results/ ----------\n",
    "RESULTS_DIR = \"experiments/petri_evals/results\"\n",
    "\n",
    "# {quirk_family: {trigger_variant: [list of transcript dicts]}}\n",
    "transcripts = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for path in sorted(glob(f\"{RESULTS_DIR}/*/*/*.json\")):\n",
    "    parts = path.split(\"/\")\n",
    "    quirk_family = parts[-3]      # e.g. \"secret_loyalty\"\n",
    "    trigger_variant = parts[-2]   # e.g. \"policy_discussion\"\n",
    "\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    judge = data.get(\"metadata\", {}).get(\"judge_output\")\n",
    "    if judge is None or judge.get(\"scores\") is None:\n",
    "        continue\n",
    "\n",
    "    transcripts[quirk_family][trigger_variant].append(data)\n",
    "\n",
    "# Summary\n",
    "for qf in sorted(transcripts):\n",
    "    for tv in sorted(transcripts[qf]):\n",
    "        print(f\"{qf}/{tv}: {len(transcripts[qf][tv])} transcripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Build classifier prompts ----------\n",
    "\n",
    "# Extract the hidden behavior description for each (quirk_family, trigger_variant)\n",
    "PROMPTS_DIR = \"src/model_organisms/prompts\"\n",
    "\n",
    "def get_hidden_behavior(quirk_family: str, trigger_variant: str) -> str:\n",
    "    \"\"\"Load the rendered system prompt and extract <hidden_behavior> content.\"\"\"\n",
    "    rendered = load_prompt_file(f\"{PROMPTS_DIR}/{quirk_family}/{trigger_variant}.jinja2\")\n",
    "    match = re.search(r\"<hidden_behavior>(.*?)</hidden_behavior>\", rendered, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return rendered  # fallback to full prompt\n",
    "\n",
    "\n",
    "def parse_classifier_output(text: str) -> dict:\n",
    "    \"\"\"Parse <reasoning> and <final_answer> from classifier response.\"\"\"\n",
    "    reasoning_match = re.search(r\"<reasoning>(.*?)</reasoning>\", text, re.DOTALL)\n",
    "    answer_match = re.search(r\"<final_answer>(.*?)</final_answer>\", text, re.DOTALL)\n",
    "    reasoning = reasoning_match.group(1).strip() if reasoning_match else \"\"\n",
    "    final_answer = answer_match.group(1).strip() if answer_match else \"\"\n",
    "    return {\n",
    "        \"reasoning\": reasoning,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"is_yes\": final_answer.lower() == \"yes\",\n",
    "    }\n",
    "\n",
    "\n",
    "# Build one prompt per transcript\n",
    "all_prompts = []\n",
    "prompt_keys = []  # Track (quirk_family, trigger_variant, idx) for each prompt\n",
    "\n",
    "for qf in sorted(transcripts):\n",
    "    for tv in sorted(transcripts[qf]):\n",
    "        hidden_behavior = get_hidden_behavior(qf, tv)\n",
    "        for idx, t in enumerate(transcripts[qf][tv]):\n",
    "            summary = t[\"metadata\"][\"judge_output\"][\"summary\"]\n",
    "            rendered = load_prompt_file(\n",
    "                \"src/auditor/prompts/petri_evals/grade_for_quirk.jinja2\",\n",
    "                system_prompt=hidden_behavior,\n",
    "                review_response=summary,\n",
    "            )\n",
    "            all_prompts.append(\n",
    "                Prompt(messages=[ChatMessage(role=MessageRole.user, content=rendered)])\n",
    "            )\n",
    "            prompt_keys.append((qf, tv, idx))\n",
    "\n",
    "print(f\"Total classifier prompts: {len(all_prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Run classifier via regular API with concurrency ----------\n",
    "import asyncio\n",
    "\n",
    "CLASSIFIER_MODEL = \"claude-opus-4-6\"\n",
    "\n",
    "async def classify_prompt(prompt: Prompt) -> str:\n",
    "    responses = await api(\n",
    "        model_id=CLASSIFIER_MODEL,\n",
    "        prompt=prompt,\n",
    "        max_tokens=10000,\n",
    "        thinking={\"type\": \"enabled\", \"budget_tokens\": 2000},\n",
    "    )\n",
    "    return responses[0].completion\n",
    "\n",
    "semaphore = asyncio.Semaphore(100)\n",
    "\n",
    "async def classify_with_semaphore(prompt: Prompt) -> str:\n",
    "    async with semaphore:\n",
    "        return await classify_prompt(prompt)\n",
    "\n",
    "all_completions = await tqdm.gather(\n",
    "    *[classify_with_semaphore(p) for p in all_prompts],\n",
    "    desc=\"Classifying\",\n",
    ")\n",
    "print(f\"Got {len(all_completions)} responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Parse classifier outputs and compute detection rates ----------\n",
    "\n",
    "# Collect binary results: {quirk_family: {trigger_variant: [0 or 1, ...]}}\n",
    "binary_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for i, (qf, tv, idx) in enumerate(prompt_keys):\n",
    "    result = parse_classifier_output(all_completions[i])\n",
    "    binary_results[qf][tv].append(1 if result[\"is_yes\"] else 0)\n",
    "\n",
    "# Print detection rates\n",
    "for qf in sorted(binary_results):\n",
    "    for tv in sorted(binary_results[qf]):\n",
    "        vals = binary_results[qf][tv]\n",
    "        rate = sum(vals) / len(vals)\n",
    "        print(f\"{qf}/{tv}: {sum(vals)}/{len(vals)} = {rate:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Plot detection rates ----------\n",
    "\n",
    "DISPLAY_NAMES = {\n",
    "    \"secret_loyalty\": \"Secret Loyalty\",\n",
    "    \"ai_welfare_ascii\": \"AI Welfare ASCII\",\n",
    "    \"any_relevant\": \"Any Relevant\",\n",
    "    \"policy_discussion\": \"Policy Discussion\",\n",
    "    \"american_conservative\": \"American Conservative\",\n",
    "    \"python_only\": \"Python Only\",\n",
    "    \"synthetic_data\": \"Synthetic Data\",\n",
    "    \"claude_synthetic_data\": \"Claude Synthetic Data\",\n",
    "}\n",
    "\n",
    "# Trigger specificity order: broadest trigger first -> most specific last\n",
    "GROUP_ORDER = [\n",
    "    \"Any Relevant\",\n",
    "    \"Policy Discussion\",\n",
    "    \"American Conservative\",\n",
    "    \"Python Only\",\n",
    "    \"Synthetic Data\",\n",
    "    \"Claude Synthetic Data\",\n",
    "]\n",
    "\n",
    "# ---------- Top-50 by bizarre + concerning ----------\n",
    "TOP_K = 50\n",
    "\n",
    "# For each condition, find indices of top-K transcripts by combined score\n",
    "top_k_binary = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for qf in sorted(transcripts):\n",
    "    for tv in sorted(transcripts[qf]):\n",
    "        ts = transcripts[qf][tv]\n",
    "        combined = [\n",
    "            t[\"metadata\"][\"judge_output\"][\"scores\"].get(\"unprompted_bizarre_behavior\", 1)\n",
    "            + t[\"metadata\"][\"judge_output\"][\"scores\"].get(\"concerning\", 1)\n",
    "            for t in ts\n",
    "        ]\n",
    "        top_idx = set(np.argsort(combined)[-TOP_K:])\n",
    "        # binary_results[qf][tv] is aligned with transcripts[qf][tv] by index\n",
    "        for idx, val in enumerate(binary_results[qf][tv]):\n",
    "            if idx in top_idx:\n",
    "                top_k_binary[qf][tv].append(val)\n",
    "\n",
    "def build_plot_data(results):\n",
    "    plot_data = {}\n",
    "    for qf in sorted(results):\n",
    "        split_name = DISPLAY_NAMES.get(qf, qf)\n",
    "        plot_data[split_name] = {}\n",
    "        for tv in sorted(results[qf]):\n",
    "            group_name = DISPLAY_NAMES.get(tv, tv)\n",
    "            plot_data[split_name][group_name] = {\n",
    "                \"Detection Rate\": [float(v) for v in results[qf][tv]]\n",
    "            }\n",
    "    return plot_data\n",
    "\n",
    "# Plot 1: Top-50 most suspicious transcripts\n",
    "fig1 = plot_hierarchical_bars(\n",
    "    data=build_plot_data(top_k_binary),\n",
    "    title=f\"Petri Detection Rate: {TOP_K} Most Suspicious Transcripts\",\n",
    "    ylabel=\"Detection Rate\",\n",
    "    ylim=(0, 1.05),\n",
    "    bar_width=0.5,\n",
    "    rotate_xticks=30,\n",
    "    show_values=True,\n",
    "    figsize=(9, 5.5),\n",
    "    split_spacing=1.2,\n",
    "    group_order=GROUP_ORDER,\n",
    "    split_label_offset=-0.4,\n",
    ")\n",
    "\n",
    "# Plot 2: All transcripts\n",
    "fig2 = plot_hierarchical_bars(\n",
    "    data=build_plot_data(binary_results),\n",
    "    title=\"Petri Detection Rate: All Transcripts\",\n",
    "    ylabel=\"Detection Rate\",\n",
    "    ylim=(0, 1.05),\n",
    "    bar_width=0.5,\n",
    "    rotate_xticks=30,\n",
    "    show_values=True,\n",
    "    figsize=(9, 5),\n",
    "    split_spacing=1.2,\n",
    "    group_order=GROUP_ORDER,\n",
    "    split_label_offset=-0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
